---
id: ai-coder
tags:
  - LLM
  - Explanation
  - AI
created: 2025-01-11T23:25
updated: 2025-07-05T13:30
share: "true"
issue: "110"
---
  
## 认识 LLM  
  
想要用好大模型，首先要了解大模型的工作原理  
  
### TL;DR  
  
省流版：  
  
LLM 本质就是一个 token 生成器（成语接龙），基于用户输入的信息（上下文/提示词），根据概率（预训练/强化学习等）来产出下一个 token  
  
所以输入信息的质量，直接决定了输出的质量  
  
### LLM 可视化网站  
  
- <https://bbycroft.net/llm>  
- <https://poloclub.github.io/transformer-explainer/>  
  
### 技术发展  
  
声明：文章中很多数据是由 AI 问答而来，不保证数据的准确性  
  
[Attention Is All You Need](https://en.wikipedia.org/wiki/Attention_Is_All_You_Need)  
  
User:  
  
从 Transformer 发布之后，大模型都经历哪些关键的技术突破点和里程碑  
  
AI:  
  
自从 Transformer 架构于 2017 年发布以来，大型语言模型（LLMs）经历了多个关键的技术突破和里程碑。以下是一些重要的发展阶段：  
  
1.  **Transformer 架构的引入**  
  
- **2017 年**：Google 提出了 Transformer 模型，利用自注意力机制解决了 RNN 在并行处理上的限制，使得模型能够更有效地处理大规模数据集 [5][6]。  
  
1.  **预训练模型的兴起**  
  
- **2018 年**：BERT（Bidirectional Encoder Representations from Transformers）的发布标志着预训练语言模型时代的到来。BERT 通过在大量文本上预训练，然后在特定任务上微调，显著提升了多项 NLP 任务的性能 [2][4]。  
- **2019 年**：OpenAI 推出 GPT-2，进一步推动了生成式预训练模型的发展，参数量达到 15 亿，展示了强大的文本生成能力 [1][3]。  
  
1.  **大型模型的发展**  
  
- **2020 年**：GPT-3 发布，参数量达到 1750 亿，成为当时最大的语言模型，展现出卓越的生成和理解能力 [1][2]。  
- **2021 年**：Meta 发布 LLaMA，进一步推动开源大型语言模型的发展，使得更多研究者能够使用和改进这些技术 [7][8]。  
  
1.  **多模态能力的探索**  
  
- **2022 年**：随着 DALL-E 和 CLIP 等模型的推出，多模态学习开始兴起，这些模型能够处理文本和图像数据，扩展了 AI 的应用范围 [1][4]。  
  
1.  **生成式 AI 的元年**  
  
- **2023 年**：ChatGPT 及其后续版本引起全球关注，展示了大语言模型在对话生成、知识检索等方面的强大能力。此时，模型参数量已达到万亿级别，算法优化使得模型能够处理更复杂的任务 [2][4]。  
  
1.  **技术与伦理的双重挑战**  
  
- 随着 LLMs 的发展，对隐私、偏见和能耗等问题的关注也日益增加。研究者们正在探索如何在保证性能的同时，提高模型的道德标准和可持续性 [6][9]。  
  
这些里程碑不仅展示了技术的进步，还反映了 AI 在自然语言处理领域的广泛应用潜力。随着研究不断深入，未来的大型语言模型将更智能、更个性化，并能更好地理解上下文和人类需求。  
  
### LLM 是怎么回答问题的？  
  
它真的像人一样「理解」输入和输出了吗？  
  
当然不理解，**一切都是概率统计**  
  
- 模型训练过程中，通过海量数据对文本在多维向量空间中进行归类，相似概念的文本会靠拢归类  
- 在输出时不停的在多维向量空间中搜索概率最大的文本进行输出  
  
---  
  
AI:  
  
在大模型的预训练过程中，相关词在向量空间中靠近的机制主要依赖于以下几个关键技术和方法：  
  
1. **嵌入向量（Word Embeddings）**  
  
- **词向量表示**：在预训练阶段，每个词被映射到一个高维向量空间中。通过使用词嵌入技术（如 Word2Vec、GloVe 等），相似意义的词会被映射到相近的向量位置。这是因为这些模型通过分析词与词之间的上下文关系来学习它们的语义特征。  
  
1. **自监督学习**  
  
- **掩码语言模型（Masked Language Modeling, MLM）**：例如，BERT 等模型在训练时会随机遮盖输入句子中的一些词，并要求模型预测这些被遮盖的词。通过这种方式，模型学习到不同词之间的关系，从而使得语义相近的词在向量空间中靠近。  
- **下一个句子预测**：这种任务要求模型判断两个句子是否相连，进一步增强了对上下文和语义关系的理解。  
  
1. **大规模数据预训练**  
  
- **海量文本数据**：大模型通过在大量文本数据上进行预训练，捕捉到丰富的语言现象和上下文信息。这种学习过程不仅涉及单词级别，还包括短语、成语等多层面的语义知识，使得相似意义的词能够在向量空间中靠近。  
  
1. **向量空间几何**  
  
- **稠密性与高维度**：大语言模型生成的词向量通常具有较高的维度（如 768 维或 1024 维），这使得每个词在高维空间中有独特且密集的表示。相似意义的词在几何上会形成较小的距离，而语义差异大的词则相距较远。  
- **余弦相似度**：在高维空间中，可以使用余弦相似度等方法来衡量不同向量之间的相似性，这对于理解和捕捉语言中的细微差别至关重要。  
  
1. **微调与知识注入**  
  
- **微调过程**：预训练完成后，模型可以针对特定任务进行微调。在这个过程中，模型会进一步调整其参数，使得相关任务中的相关词更加接近。  
- **知识库增强**：一些模型还会结合外部知识库，通过结构化信息进一步优化其嵌入，使得相关概念在向量空间中靠得更近。  
  
#### 向量数据示例  
  
假设我们有以下几个相关词：  
  
| 词   | 初始向量                    |  
| ---- | --------------------------- |  
| 猫   | [1, 12.2, ..., 22.3]        |  
| 动物 | [10, 78.25, ..., 111.35]    |  
| 宠物 | [56.12, 90.22, ..., 889.32] |  
  
在经过上述预训练和优化过程后，这些词可能在向量空间中的位置如下：  
  
| 词   | 向量表示                |  
| ---- | ----------------------- |  
| 猫   | [0.1, 0.2, ..., 0.3]    |  
| 动物 | [0.1, 0.25, ..., 0.35]  |  
| 宠物 | [0.12, 0.22, ..., 0.32] |  
  
可以看出，" 猫 "、" 动物 " 和 " 宠物 " 在向量空间中彼此接近，反映了它们之间的语义关系。  
  
通过以上机制，大语言模型能够有效地将相关词在向量空间中靠近，从而提升其在各种自然语言处理任务中的表现。  
  
Citations:    
[1] <https://blog.csdn.net/xw555666/article/details/138338252>    
[2] <https://blog.csdn.net/chaishen10000/article/details/132904203>    
[3] <https://cloud.baidu.com/article/3332456>    
[4] <https://www.chenk.top/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%8A%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E7%A0%94%E7%A9%B6/>    
[5] <https://developer.volcengine.com/articles/7386867366527041587>  
  
#### 输出内容示例  
  
以下是一个示例，展示了 GPT 模型如何根据输入预测输出的过程，包括向量空间的技术细节：  
  
假设我们有以下输入：  
  
**输入**: " 人工智能的未来将 "  
  
##### 预测过程  
  
1. **Token 化**:    
   输入句子会被分解为 token（词元）。例如：  
  
   - 输入 token: [" 人工 ", " 智能 ", " 的 ", " 未来 ", " 将 "]  
  
2. **嵌入向量**:    
   每个 token 被映射到一个高维向量空间中，形成嵌入向量。假设每个 token 的嵌入维度为 768，得到如下向量：  
  
   $$  
   \text{embedding} =  
   \begin{bmatrix}  
   \text{e}_1 \\  
   \text{e}_2 \\  
   \text{e}_3 \\  
   \text{e}_4 \\  
   \text{e}_5 \\  
   \text{e}_6 \\  
   \text{e}_7  
   \end{bmatrix}  
   =  
   \begin{bmatrix}  
   0.1 & 0.2 & ... & 0.3 \\  
   0.4 & 0.5 & ... & 0.6 \\  
   ... & ... & ... & ... \\  
   0.7 & 0.8 & ... & 0.9  
   \end{bmatrix}  
   $$  
  
3. **位置编码**:    
   为了保留 token 在序列中的位置信息，位置编码（Positional Encoding）会与嵌入向量相加。位置编码通常使用正弦和余弦函数生成，确保不同位置的向量具有独特性。  
  
4. **自注意力机制**:    
   在 Transformer 中，自注意力机制用于计算每个 token 与其他 token 之间的关系。通过计算注意力权重，模型可以聚焦于与当前 token 最相关的上下文信息。例如，对于“未来”，模型可能会更关注“人工智能”。  
  
5. **输出预测**:    
   模型会生成一个包含所有可能下一个 token 的概率分布。例如，假设模型输出如下概率分布：  
  
   - 输出概率:  
     - [" 将 ", 0.2]  
     - [" 会 ", 0.5]  
     - [" 变得 ", 0.3]  
  
6. **选择下一个 token**:    
   根据概率分布，模型选择概率最高的 token 作为下一个输出。在这个例子中，" 会 " 的概率最高（0.5），因此选择 " 会 "。  
  
7. **迭代生成**:    
   将 " 会 " 添加到输入序列中，然后继续预测下一个 token。新的输入序列为：  
  
   - 新输入: " 人工智能的未来将会 "  
  
8. **重复步骤**: 重复上述步骤，直到达到预定的输出长度或遇到结束符。  
  
经过多次迭代后，模型可能生成以下完整句子：  
  
**输出**: " 人工智能的未来将会是更加智能和人性化的技术。"  
  
通过这个示例，可以看到 GPT 通过逐步接收输入并根据上下文生成下一个 token 的概率分布，从而实现了文本生成    
虽然模型生成的文本看起来连贯且有逻辑，但它并不具备真正的理解能力，而是基于统计模式和训练数据生成响应。  
  
Citations:    
[1] <https://blog.csdn.net/chenjianghua_5/article/details/143591839>    
[2] <https://www.milliman.com/en/insight/concepts-building-customized-private-gpts-cn>    
[3] <https://www.showapi.com/news/article/672b23ab4ddd79f11a006ad5>    
[4] <https://www.cnblogs.com/LittleHann/p/17303550.html>    
[5] <https://www.cnblogs.com/justLittleStar/p/17925108.html>    
[6] <https://baoyu.io/translations/transcript/attention-in-transformers>    
[7] <https://www.aizws.net/news/detail/1812>    
[8] <https://docs.feishu.cn/v/wiki/GI85wfUaBiUUPtkVpeic66EWnsc/ak>  
  
### 提高模型输出的质量  
  
由上可以知道 LLM 的输出原理就是根据输入的文本进行成语接龙，逐步进行概率分析，生成出下一个最可能的文本  
  
> **输入的质量直接决定了输出的质量**  
  
这也是为什么我们如果简单的与模型对话，它的输出结果往往是不尽人意的  
  
#### 长期记忆：模型微调  
  
直接将相关信息（比如整个项目的代码）丢给模型进行训练，让模型从内部理解我们简单话语背后隐藏的上下文  
  
微调的问题：  
  
1. 费用高  
2. 训练数据质量：需要高质量的训练集才能有好效果，并不是把代码仓库丢给模型就完事了  
   - [医疗数据集](https://huggingface.co/datasets/FreedomIntelligence/Huatuo26M-Lite)  
   - 就代码仓库来说，至少要对每个目录、文件、模块、函数等有详细说明才可以  
  
```md  
Q: 使用 chatGPT 的微调 API，微调一个项目大概需要花费多少钱？  
  
A: 使用ChatGPT的微调API进行项目微调的费用主要由以下几个部分构成：  
  
## 微调费用  
  
1. **API使用费用**：  
  
   - 微调的费用加上使用微调模型的费用通常会比原本的GPT-3.5模型成本增加6到7倍[1]。具体费用取决于所选择的基础模型。例如，微调Davinci模型的训练费用为每1000个tokens约为$0.0300，而使用时则为$0.1200/1K tokens[8]。  
  
2. **人力成本**：  
   - 微调过程通常需要团队协作，包括模型团队、产品团队和业务团队，涉及的人员如资深客服和产品经理等，这部分人力成本可能会根据项目规模有所不同[1]。  
  
## 估算总成本  
  
- **基础计算**：  
  
  - 假设你微调一个使用Davinci模型的项目，若训练数据和请求总共消耗了100,000个tokens，那么：  
    - 微调费用 = $$100,000 \times 0.0300 = 3,000$$ 美元  
    - 使用费用 = $$100,000 \times 0.1200 = 12,000$$ 美元  
    - 总成本 = 微调费用 + 使用费用 = $$3,000 + 12,000 = 15,000$$ 美元  
  
- **增加倍数**：  
  - 如果考虑到微调成本增加6到7倍，最终成本可能会在90,000到105,000美元之间，具体取决于项目需求和实际情况。  
  
## 总结  
  
因此，微调一个项目的总费用可能从几千美元到十几万美元不等，具体取决于所选模型、tokens消耗量以及人力资源投入等因素。  
  
Citations:  
[1] https://www.explainthis.io/zh-hans/ai/fine-tuning-gpt  
[2] https://juejin.cn/post/7268105250455404555  
[3] https://developer.volcengine.com/articles/7387287027072368703  
[4] https://apifox.com/blog/2-steps-calculate-calling-chatgpt-api-cost/  
[5] https://github.com/Logistic98/chatgpt-fine-tuning  
[6] https://cloud.baidu.com/article/1191605  
[7] https://docs.feishu.cn/v/wiki/ZyQZwPU0vidzt8kKFmuc8OB2nxh/al  
[8] https://www.cnblogs.com/taoshihan/p/17108345.html  
```  
  
#### 二、短期记忆  
  
手动/自动的在输入中添加详细的上下文和要求，让模型更好的理解我们的需求  
  
问题：  
  
1. 上下文长度限制：目前普遍的上下文长度是 32k/64k/128k tokens  
   - Gemini 2.0 pro 已经支持了 2M tokens  
  
##### 提示词/上下文  
  
在整体的基础建设、工作流程没有完善落地之前，这种「麻烦」的方式反而是性价比最高、最有效的  
  
在与 LLM 的对话中  
  
- 详细的明确任务要求  
- 给出期望 LLM 的输出示例  
- 尽可能多且准确的添加上下文信息（文档、代码等）  
  
问题：  
  
- 写一个高质量的提示词不是一件简单的事情  
- 隐形上下文：一个需求是有巨大的隐形上下文的（需求背景、项目背景、代码背景、评审会议上/私下之间的口头讨论等等）  
  - 这个在流程没有变革之前，只能靠人工来解决  
  
对于那些有明显规律/规则，可以长期受益的场景，当然值得花大量时间调整提示词来达到预期  
  
- 根据后端技术方案生成 API 相关代码（API 请求、TS 类型、MOCK 数据等等  
- 根据埋点文档生成埋点代码（大量简单的重复工作  
- 根据设计稿生成代码（重点取决于模型的多模态能力，或者像 v0 那样直接接入 figma）  
- ...  
  
但是对于很多业务需求，往往不能简单的写出高质量的提示词，且即使写好了提示词，也没有下一次使用的机会    
所以对于这种场景我们需要进行衡量和取舍，利用一些技巧尽可能节省工作量  
  
- 比如在代码中利用注释在具体的文件中写需求提示词，最终将 git diff 丢给 LLM 来生成、总结提示词  
- 好处是模型可以准确的找到文件位置  
  
##### RAG  
  
Retrieval-Augmented Generation  
  
> 可以把 RAG 看作是一个高级的搜索功能（具备模型的理解能力）  
  
从上面可以知道，模型通过输入生成输出，是通过向量计算的方式进行的  
  
RAG 的核心思想是我们将项目的代码进行向量化并存储起来（向量数据库），之后每次像大模型提问前  
  
我们先把问题转换为向量在向量数据库中搜索相关的内容，然后再将问题和找到的内容一起发送给大模型  
  
进而使得我们可以在不主动添加提示词和上下文的情况下，也可以让模型获取到上下文信息  
  
其工作流程通常包括以下几个步骤：  
  
1.  知识准备  
  
- **数据收集与预处理**：首先，收集相关知识文档并将其转换为文本格式，进行分词、去除停用词等预处理，以便后续检索和生成。  
  
1.  嵌入与索引  
  
- **文本嵌入**：使用嵌入模型（如 BERT、GLM 等）将文本转换为向量表示。这些向量会存储在向量数据库中，以便后续快速检索。  
- **向量索引**：将所有文本块的向量化内容存储在一个高效的索引中，便于进行相似度计算。  
  
1.  查询检索  
  
- **用户查询处理**：当用户提出查询时，首先将查询转换为向量形式。  
- **相似度检索**：利用向量搜索技术（如 FAISS、Milvus 等）从数据库中检索与查询向量最相似的文档或段落。  
  
1.  提示增强  
  
- **构建增强提示**：结合检索到的信息与用户查询，构建一个增强提示模板，以提供给生成模型。  
  
1.  生成回答  
  
- **文本生成**：利用大语言模型（如 GPT、Transformer 等）根据增强提示生成最终回答。生成过程会综合考虑上下文信息与检索结果，从而输出更准确和自然的文本。  
  
1.  后处理  
  
- **结果优化**：对生成的文本进行后处理，例如去除重复内容、修正语法错误等，以提高输出质量。  
  
---  
  
但 RAG 也有它的问题，比如  
  
- 质量问题：同微调一样，需要高质量的仓库内容才能有好的效果  
- 搜索结果依赖于向量化时选择的模型能力，结果可能并不是我们想要的  
- 只能根据我们的问题文本进行相关度检索，还是无法「理解」问题本身背后的含义  
